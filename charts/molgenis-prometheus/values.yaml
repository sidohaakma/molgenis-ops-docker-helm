prometheus:
  alertmanager:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi
    configMapOverrideName: "alertmanager-configmap"

  kubeStateMetrics:
    enabled: false
    nodeSelector:
      deployPod: "false"
    resources:
      limits:
        cpu: 10m
        memory: 64Mi
      requests:
        cpu: 10m
        memory: 64Mi

  nodeExporter:
    enabled: false
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 100m
        memory: 30Mi

  pushgateway:
    nodeSelector:
      deployPod: "true"
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi

  server:
    nodeSelector:
      deployPod: "true"
    extraSecretMounts:
      - name: scrape-secrets
        secretName: scrape-secrets
        mountPath: /etc/secrets
        readOnly: true
    resources:
      limits:
        cpu: 1
        memory: 6Gi
      requests:
        cpu: 1
        memory: 6Gi
    configMapOverrideName: "serverfiles-configmap"
  configmapReload:
    resources:
      limits:
        cpu: 100m
        memory: 64Mi
      requests:
        cpu: 100m
        memory: 64Mi
  #serverFiles:
    # alerts:
    #   groups:
    #     - name: virtualmachines
    #       rules:
    #         - alert: "Instance or node_exporter Down"
    #           expr: up == 0
    #           annotations:
    #             severity: "ERROR"
    #             summary: "Instance or node_exporter Down"
    #             message: "Instance or node_exporter of {{$labels.instance}} is down\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: OutOfMemory
    #           expr: node_memory_MemFree / node_memory_MemTotal * 100 < 10
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Out of memory (instance {{ $labels.instance }})"
    #             message: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualNetworkThroughputIn
    #           expr: sum by (instance) (irate(node_network_receive_bytes[2m])) / 1024 / 1024 > 100
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual network throughput in (instance {{ $labels.instance }})"
    #             message: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualNetworkThroughputOut
    #           expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) / 1024 / 1024 > 100
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual network throughput out (instance {{ $labels.instance }})"
    #             message: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualDiskReadRate
    #           expr: sum by (instance) (irate(node_disk_read_bytes[2m])) / 1024 / 1024 > 50
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual disk read rate (instance {{ $labels.instance }})"
    #             message: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualDiskWriteRate
    #           expr: sum by (instance) (irate(node_disk_written_bytes[2m])) / 1024 / 1024 > 50
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual disk write rate (instance {{ $labels.instance }})"
    #             message: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: OutOfDiskSpace
    #           expr: node_filesystem_free_bytes{mountpoint ="/rootfs"} / node_filesystem_size_bytes{mountpoint ="/rootfs"} * 100 < 10
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Out of disk space (instance {{ $labels.instance }})"
    #             message: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: OutOfInodes
    #           expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint ="/rootfs"} * 100 < 10
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Out of inodes (instance {{ $labels.instance }})"
    #             message: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualDiskReadLatency
    #           expr: rate(node_disk_read_time_ms[1m]) / rate(node_disk_reads_completed[1m]) > 100
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual disk read latency (instance {{ $labels.instance }})"
    #             message: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: UnusualDiskWriteLatency
    #           expr: rate(node_disk_write_time_ms[1m]) / rate(node_disk_writes_completed[1m]) > 100
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Unusual disk write latency (instance {{ $labels.instance }})"
    #             message: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: HighCpuLoad
    #           expr: 100 - (avg by(instance) (irate(node_cpu{mode="idle"}[5m])) * 100) > 80
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "High CPU load (instance {{ $labels.instance }})"
    #             message: "CPU load is > 80%\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: ContextSwitching
    #           expr: rate(node_context_switches[5m]) > 1000
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Context switching (instance {{ $labels.instance }})"
    #             message: "Context switching is growing on node (> 1000 / s)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #         - alert: SwapIsFillingUp
    #           expr: (1 - (node_memory_SwapFree / node_memory_SwapTotal)) * 100 > 80
    #           for: 5m
    #           annotations:
    #             severity: warning
    #             summary: "Swap is filling up (instance {{ $labels.instance }})"
    #             message: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\nBranch: {{ $labels.branch }}\nProject: {{ $labels.project }}\nDTAP: {{ $labels.dtap }}"
    #     - name: nodes
    #       rules:
    #         - alert: "Node Down"
    #           expr: up{job="kubernetes-nodes"} == 0
    #           annotations:
    #             miqTarget: "ContainerNode"
    #             severity: "ERROR"
    #             message: "Node {{$labels.instance}} is down"
    #     - name: pods
    #       rules:
    #         - alert: "Pod OOMKilled"
    #           expr: kube_pod_container_status_terminated_reason{reason='OOMKilled'} == 1
    #           annotations:
    #             severity: "WARN"
    #             message: "Pod {{$labels.container}} in namespace {{$labels.namespace}} got OOMKilled."
    # rules: {}
    # prometheus.yml:
    #   rule_files:
    #     - /etc/config/rules
    #     - /etc/config/alerts
    #   scrape_configs:
    #     - job_name: 'masterBranchNodes'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/*TargetsAcquired.yml
    #     - job_name: '30BranchNodes'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/30TargetsAcquired.yml
    #     - job_name: '20BranchNodes'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/20TargetsAcquired.yml
    #     - job_name: '10BranchNodes'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/10TargetsAcquired.yml
    #     - job_name: '01BranchNodes'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/01TargetsAcquired.yml
    #     - job_name: 'developmentPods'
    #       file_sd_configs:
    #         - files:
    #           - /etc/configProm/developmentpods.yml
  extraScrapeConfigs: |
    - job_name: prometheus
      static_configs:
        - targets:
            - localhost:9090
    # A scrape configuration for running Prometheus on a Kubernetes cluster.
    # This uses separate scrape configs for cluster components (i.e. API server, node)
    # and services to allow each to use different authentication configs.
    #
    # Kubernetes labels will be added as Prometheus labels on metrics via the
    # `labelmap` relabeling action.
    # Scrape config for API servers.
    #
    # Kubernetes exposes API servers as endpoints to the default/kubernetes
    # service so this uses `endpoints` role and uses relabelling to only keep
    # the endpoints associated with the default/kubernetes service using the
    # default named port `https`. This works for single API server deployments as
    # well as HA API server deployments.
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
        - role: endpoints
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      # Keep only the default/kubernetes service endpoints for the https port. This
      # will add targets for each API server which Kubernetes adds an endpoint to
      # the default/kubernetes service.
      relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
    - job_name: 'kubernetes-nodes'
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
    - job_name: 'kubernetes-nodes-cadvisor'
      # Default to scraping over https. If required, just disable this or change to
      # `http`.
      scheme: https
      # This TLS & bearer token file config is used to connect to the actual scrape
      # endpoints for cluster components. This is separate to discovery auth
      # configuration because discovery & scraping are two separate concerns in
      # Prometheus. The discovery auth config is automatic if Prometheus runs inside
      # the cluster. Otherwise, more config options have to be provided within the
      # <kubernetes_sd_config>.
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # If your node certificates are self-signed or use a different CA to the
        # master CA, then disable certificate verification below. Note that
        # certificate verification is an integral part of a secure infrastructure
        # so this should only be disabled in a controlled environment. You can
        # disable certificate verification by uncommenting the line below.
        #
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
        - role: node
      # This configuration will work only on kubelet 1.7.3+
      # As the scrape endpoints for cAdvisor have changed
      # if you are using older version you need to change the replacement to
      # replacement: /api/v1/nodes/${1}:4194/proxy/metrics
      # more info here https://github.com/coreos/prometheus-operator/issues/633
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        # Only for Kubernetes ^1.7.3.
        # See: https://github.com/prometheus/prometheus/issues/2916
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      metric_relabel_configs:
        - action: replace
          source_labels: [id]
          regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
          target_label: rkt_container_name
          replacement: '${2}-${1}'
        - action: replace
          source_labels: [id]
          regex: '^/system\.slice/(.+)\.service$'
          target_label: systemd_service_name
          replacement: '${1}'
    # Scrape config for service endpoints.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: 'kubernetes-service-endpoints'
      kubernetes_sd_configs:
        - role: endpoints
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: instance
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_name
    - job_name: 'prometheus-pushgateway'
      honor_labels: true
      kubernetes_sd_configs:
        - role: service
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
          action: keep
          regex: pushgateway
    # Example scrape config for probing services via the Blackbox Exporter.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/probe`: Only probe services that have a value of `true`
    - job_name: 'kubernetes-services'
      metrics_path: /probe
      params:
        module: [http_2xx]
      kubernetes_sd_configs:
        - role: service
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
          action: keep
          regex: true
        - source_labels: [__address__]
          target_label: __param_target
        - target_label: __address__
          replacement: blackbox
        - source_labels: [__param_target]
          target_label: instance
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: kubernetes_name
    # Example scrape config for pods
    #
    # The relabeling allows the actual pod scrape endpoint to be configured via the
    # following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
secret:
  GITHUBTOKEN: "xxxx"
environment: "development"
